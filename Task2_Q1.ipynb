{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Task2_Q1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnNqb4FwLxTy"
      },
      "source": [
        "# Task 2 CNN\n",
        "### Question 1\n",
        "First we import all the necessary libraries \n",
        "- os - for getting the image path \n",
        "- numpy - for handling data \n",
        "-cv2 - for resizing image \n",
        "-scikit-learn - for train test split and measuring accuracy \n",
        "-tensorflow - for CNN \n",
        "-datetime - for saving the model \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooRjFM8NJBun"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv1D, MaxPooling1D\n",
        "from sklearn.metrics import accuracy_score\n",
        "import datetime\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PYyMqITMkl1"
      },
      "source": [
        "Now, we create ImageDataGenerator object for image augmentation. We only have around 40 images of each sample which is very less for training a neural network. So, I used Image Augmentation(which generates more images by scaling, rotating, etc. the original images) for generating more samples so that the model doesn't have the problem of overfitting. \n",
        "\n",
        "The images are increased from 40 per category to around 280 per category, making the total images around 17000."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5NBYNxvLhEE"
      },
      "source": [
        "datagen = ImageDataGenerator(\n",
        "        rotation_range=40,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=False, \n",
        "        vertical_flip=False,\n",
        "        fill_mode='nearest')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "triYKuJENrRW"
      },
      "source": [
        "This code block uses the os library to get the list of all the folders in the train folder(Sample 001, Sample 002...) and stores them in a list names files.\n",
        "\n",
        "This will be used while augmentation and while importing the images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAF9TUniJJoC"
      },
      "source": [
        "files = []\n",
        "for f in os.listdir('train/'):\n",
        "    if os.path.isdir(f'train/{f}'):\n",
        "        files.append(f'train/{f}')\n",
        "files.sort()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MwXYKWzOHT9"
      },
      "source": [
        "This block of code creates the augmented images. It create 6 images from each sample, thus images become 280(40*6 + 40(original))."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1bBScXuLpQ0"
      },
      "source": [
        "for i in range(1):\n",
        "    img_path = os.listdir(files[i])\n",
        "    img_path.sort()\n",
        "    label = i\n",
        "    for j in img_path:\n",
        "        path = f'{files[i]}/{j}'\n",
        "        img = load_img(path)  # this is a PIL image\n",
        "        x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)\n",
        "        x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)\n",
        "    a = 0\n",
        "    for batch in datagen.flow(x, batch_size=1, save_to_dir=files[i], save_prefix=f'{a}', save_format='png'):\n",
        "        a += 1\n",
        "        if a > 5:\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeCetF5TSS34"
      },
      "source": [
        "Now, we import all the images and their labels and store them in X and y respectively. To import images, I have used OpenCV and imported them in grayscale."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxqavYsCJQWZ"
      },
      "source": [
        "X = []\n",
        "y = []\n",
        "for i in range(len(files)):\n",
        "    img_path = os.listdir(files[i])\n",
        "    img_path.sort()\n",
        "    label = i\n",
        "    for j in img_path:\n",
        "        path = f'{files[i]}/{j}'\n",
        "        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
        "        X.append(img)\n",
        "        y.append(label)\n",
        "size = len(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DkuWNjZaqqd"
      },
      "source": [
        "Now, we preprocess the data. I have reduced the image size to 50 x 50 for faster training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BY9W9URhJVfx"
      },
      "source": [
        "height = 50\n",
        "width = 50\n",
        "for i in range(size):\n",
        "    img = X[i]\n",
        "    img_resize = cv2.resize(img, (height, width))\n",
        "    X[i] = img_resize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3vwqEqua1aG"
      },
      "source": [
        "This block does a few things.\n",
        "\n",
        "First line, changes X from list to a numpy array. Next line, scales the data, it changes the data range from 0-255 to 0-1, this sometimes increases the accuracy and it's always a good practice to scale data.\n",
        "\n",
        "Next, we split the data into test and training set. We use 10% of the data for testing(validation and testing).\n",
        "\n",
        "Last two lines are used to convert y_train and y_test samples to categorical samples by one hot encoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ul7AF-aPJgoU"
      },
      "source": [
        "X = np.stack(X)  \n",
        "X = X/255.0\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\n",
        "y_train = to_categorical(y_train, dtype = 'int64') # One hot encoding\n",
        "y_test1 = to_categorical(y_test, dtype = 'int64') # One hot encoding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obGftlYSbUNR"
      },
      "source": [
        "Now we define the model. By trial and error and all the different approaches mentioned in the experiment log, I have finalised this model. It gives 96% training accuracy and 89% testing accuracy.\n",
        "\n",
        "I am using 3 convolution layers, 3 maxpooling layers, 2 dropout layers for preventing overfitting, two dense layers for getting the features and for output, and one flatten layer. \n",
        "\n",
        "relu activation is used for all layers except output layer which uses softmax. \\\n",
        "Loss function used is Categorical Crossentropy.\\\n",
        "Adam optimizer is used after testing with RMSProp, Adam and SGD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJisQ3NBJjJ3"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv1D(32,kernel_size = (5), padding ='valid',activation = 'relu', input_shape = (height, width)))\n",
        "model.add(MaxPooling1D(pool_size = (2)))\n",
        "\n",
        "model.add(Conv1D(64,kernel_size = (5), padding ='same',activation = 'relu'))\n",
        "model.add(MaxPooling1D(pool_size = (2)))\n",
        "\n",
        "model.add(Conv1D(64,kernel_size = (3), padding ='same',activation = 'relu'))\n",
        "model.add(MaxPooling1D(pool_size = (2)))\n",
        "\n",
        "model.add(Dropout(0.05))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Dense(62, activation = 'softmax'))\n",
        "\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "print(\"Model Defined. Training now.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJTgxhqVeH4K"
      },
      "source": [
        "Now we train the model with 60 epochs. I have seen that by 50 epochs the model reaches convergence as its accuracy stops increasing and pretty much occilates around 95%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e7f56XzJn81"
      },
      "source": [
        "model.fit(X_train, y_train, epochs = 60, validation_data=(X_test, y_test1))\n",
        "print(\"Model trained!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3X95pE_iecwv"
      },
      "source": [
        "Now, we predict the test samples and store them in y_pred, then we convert them back from categorical to number, and we use accuracy_score metric to judge our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5y7b2e1PJpbp"
      },
      "source": [
        "y_pred = model.predict(X_test)\n",
        "idx = np.argmax(y_pred, axis = 1)\n",
        "\n",
        "X_test = np.stack(X_test)  \n",
        "y_test = np.stack(y_test)\n",
        "results = accuracy_score(y_test, idx)\n",
        "print(\"Accuracy = \", results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJkl1zdhetU-"
      },
      "source": [
        "This block of code saves the model in the format(model_datetime of creation_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAbMNn7VJtO5"
      },
      "source": [
        "time = datetime.datetime.now()\n",
        "model.save(f\"model_{time}_{int(100*results)}.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}